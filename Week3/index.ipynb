{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSc HI DTP Module: Week 3 workshop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarising, Analysing and Communicating Data\n",
    "In the lectures for this week, we have explored the data landscape, which can inform stage (iii) of the design cycle: i.e. what data will we collect/obtain in order to address the problem statement and monitor /predict the performance of the digital intervention?\n",
    "\n",
    "In this workshop, we take our exploration of the data landscape further and explore ways of summarising (potentially complex) health data, so that we can communicate with different stakeholders. This will also allow you to gain an appreciation of which \"type\" might be most appropriate in different data contexts.\n",
    "\n",
    "To achieve this goal, we will be running some analyses on real-data using the R statistical programming language. _However, having a detailed understanding of the R code is not the aim of this activity!_ In fact, there is __no requirement to even look at the code if you do not wish to__! R is used here simply as the vehicle for obtaining results. Interested readers might wish to refer to this (online) book on R, and in particular chapter 9 that covers some of the material from this week: https://rc2e.com/. However, this is optional reading. \n",
    "\n",
    "Our primary goal is to examine how the general principles of the design cycle can be used to decide on when different types of statistical analyses would be appropriate in different digital transformation projects. We will do this by examining the outcome from some analyses, and critically viewing what they tell us about the data. \n",
    "\n",
    "### Notes about the software used in this workshop\n",
    "You will notice that this session is running using Jupyter notebooks. While, in practice, we might use R directly for analysis, we make use of Jupyter notebooks here since this allows you to run the analysis without having to install any software on your local machines.\n",
    "\n",
    "You will be guided through the code syntax at each stage, to allow you to start to become familiar with how to change the code according to your own problems/research questions. Again, this workshop is not intended as a comprehensive guide to coding and statistical analysis. To reiterate, you could easily skip over the meaning of the code if you wish: focus instead on the what inference we can draw from different types of analysis, and therefore which ones you might design into your DTP case studies (and future projects).\n",
    "\n",
    "Please refer to the short tutorial video on Moodle for how to use Jupyter notebooks. In short, you will see a mixture of text and R code, with the later included inside grey 'cells' - we call these code cells. When you click inside a code cell, you will be able to run the associated code using the \"Run\" box in the menu bar at the top of the screen (alternatively select from the menus: Cell -> Run Cells). \n",
    "\n",
    "__IMPORTANT: As you work through this workshop, run each code cell in the order they appear: you will also have space to write your own code if you wish.__\n",
    "\n",
    "## \"Types\" of Statistical Analysis\n",
    "\n",
    "Broadly speaking, there are three broad “types” of common research questions when analysing data: \n",
    "\n",
    "1) Description\n",
    "\n",
    "2) Causation\n",
    "\n",
    "3) Regression/ Prediction\n",
    "\n",
    "The type of statistical analysis largely depends on the underlying problem statement (i.e. stage i of our design cycle), the design/plan of the study and digital intervention (i.e. stage ii of our design cycle) and the type of data/outcomes we plan to analyse, drawing on our understanding of the data-landscape (i.e. stage iii of our design cycle).\n",
    "\n",
    "In this workshop, we will focus on type 1 (description), and we will cover the other types in week 4.\n",
    "\n",
    "## The Data\n",
    "\n",
    "For this activity, we will be using a dataset called \"Framingham\". The Framingham Heart Study is a long term prospective study of the etiology of cardiovascular disease among a population of participants in the community of Framingham, Massachusetts. The Framingham Heart Study was a landmark study in epidemiology in that it was the first prospective study of cardiovascular disease and identified the concept of risk factors and their joint effects. The study began in 1948, and 5209 subjects were initially enrolled in the study. Participants have been examined biannually since the inception of the study and all subjects are continuously followed through regular surveillance for cardiovascular outcomes. Clinic examination data has included cardiovascular disease risk factors and markers of disease such as blood pressure, blood chemistry, lung function, smoking history, health behaviours, ECG tracings, Echocardiography, and medication use. Through regular surveillance of area hospitals, participant contact, and death certificates, the Framingham Heart Study reviews and adjudicates events for the occurrence of Angina Pectoris, Myocardial Infarction, Heart Failure, and Cerebrovascular disease.\n",
    "\n",
    "For this practical activity, we will be exploring an anonymous subset/extract of these data.\n",
    "\n",
    "Further details of the dataset can be found on the Moodle page for this module.\n",
    "\n",
    "### Coding\n",
    "Thinking about the concepts covered in the data-landscape lecture from this week, we would regard this as structured data. Some of the outcomes are extracted from coded medical records. These codes are then turned into the columns of data that you see below. This mapping is done using pre-specified codelists, which tell us the set of codes (e.g. ICD-9/10 codes) that correspond to a \"yes\" in the cardiovascular disease column. This mapping is common when working with health data, and occurs prior to analysis.\n",
    "\n",
    "### Optional reading\n",
    "For those interested, you might wish to read a couple of papers that have used these data (avaliable on the optional reading list for this week): these are \n",
    "\n",
    "- Staerk et al. Lifetime risk of atrial fibrillation according to optimal, borderline, or elevated levels of risk factors: cohort study based on longitudinal data from the Framingham Heart Study BMJ 2018; 361:k1453\n",
    "\n",
    "and\n",
    "\n",
    "- Satizabal et al. Incidence of Dementia over Three Decades in the Framingham Heart Study. N Engl J Med 2016; 374:523-532\n",
    "\n",
    "## Load the data\n",
    "\n",
    "To begin exploring these data, we first need to load it into the session. The data are stored in csv format, and the R code to load the data is as follows (noting, for the final time, that there is no expectation for you to delve deep into R coding in this activity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frmgham_data <- read.csv(\"frmgham2.csv\", header = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meaning of this code is that it takes the Framingham csv file and assigns it (the \"<-\" symbol) to a variable called frmgham_data: in other words, we have loaded the data, and we can analyse the data by applying R functions to the frmgham_data variable. \n",
    "\n",
    "We can take a look at the first few rows of the data and see the size of the data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(frmgham_data) #shows the first 6 rows of the dataset in tabular format\n",
    "dim(frmgham_data) #shows the dimensions for the data: first number that is output is number of rows, then number of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the code above, you should see two sets of output appear on screen. The top part of this output shows the first 6 rows of data, with each variable given in the columns (note this is similar output to what one might see in Excel, for example). The second part of the output tells us that there are 11627 rows of data and 39 columns.\n",
    "\n",
    "Note from the view of the data that some patients (RANDID gives a randomly-assigned patient ID) appear on multiple rows. This is because the study design is a longitudinal study, where an individual was followed-up at multiple timepoints and data was collected at each visit. Therefore, each row of data is a \"visit\". Further details of the dataset can be found on the Moodle page for this module.\n",
    "\n",
    "## Descriptive statistics\n",
    "In order to start turning data into evidence for different \"buisness needs\" and stakeholders, we need a way of summarising key attributes of the data. Data summary is usually the first step in all project proposals/ plans (in terms of analytical methods), so it is important to build an appreciation of common data summary methods. \n",
    "\n",
    "A principal role of studying health data is to describe and explain differences in the distribution of disease or other health outcomes of interest between populations. Health data studies often generate a large volume of data. Summarising this can help draw out patterns and results. There are four principal methods used to summarise data:\n",
    "\n",
    "1)\tUsing simple frequencies - the number of times something occurs in a data set. This makes it possible to determine if the specific value we are interested in is unusual or common.\n",
    "\n",
    "2)\tMeasures of location\n",
    "\n",
    "- Mean - also known as the average\n",
    "\n",
    "- Median - the middle value in the ranked observations, used when the data is asymmetric.\n",
    "\n",
    "- Mode - the value of the random variable in the sample which occurs with the highest frequency.\n",
    "\n",
    "3)\tMeasures of spread\n",
    "\n",
    "- Range - the difference between the largest and smallest observations\n",
    "\n",
    "- Interquartile Range - describes the middle half of the data, between the 25th and 75th centiles, reducing the effect of outlying observations\n",
    "\n",
    "- Variance and standard deviation\n",
    "\n",
    "4)\tGraphical methods\n",
    "\n",
    "- Box plots - visual presentation of the quartiles of the data\n",
    "\n",
    "- Histograms - graphical presentation of the shape of a data distribution, which we look at later.\n",
    "\n",
    "At the study design stage, deciding which of these you will use depends on the types of data you plan to collect in your study. In other words, our decision depends on whether the data/variables are continuous (measures that can take any number), counts (measurements that can only take integer values: i.e. 0, 1, 2,...), or categorical (measures that only take one of a number of levels: e.g. blood type of A, B, AB or O). \n",
    "\n",
    "__Activity: At this point in the workshop, consider your DTP case study: what types of data do you plan to collect? Which of the above summary measures do you think would be appropriate? Make a note of these, as you begin to draft the statistical analysis section of your portfolio.__\n",
    "\n",
    "## Practical Exploration of Framingham data\n",
    "To build an understanding of what types of descriptive statistics might be appropriate in different situations, we will now apply some of the above methods to the Framingham dataset so you can see what outputs/interpretations we can draw. To make the interpretation easier, we will consider only the first \"visit\" for each patient - this is commonly termed the baseline summary. We will apply a filer to the dataset, to extract each individual's first set of measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frmgham_1visit_data <- frmgham_data[!duplicated(frmgham_data$RANDID),] #This row returns only the first observation for each ID\n",
    "head(frmgham_1visit_data) #output first 6 rows of data\n",
    "dim(frmgham_1visit_data) #look at dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the filter, we again explore the first 6 rows of data and the size of the data. After running the above code cell, we now see that each row of the dataset is each individual's first visit (baseline), which has reduced the size of the dataset to 4434 rows (i.e. 4434 patients in our dataset, since now one row corresponds to one person), with 39 variables still. \n",
    "\n",
    "In terms of summarising this data to different stakeholders, the above is not particularly informative, and certianly doesnt allow us to address any realistic problem statements. Therefore, we might start by calculating measures of location and spread (using the terminology from the \"Descriptive statistics\" section above).\n",
    "\n",
    "Note that such summaries of data are usually the starting point for most projects/ problem statements. At the very least, they allow one o understand the data and spot any potential data errors.\n",
    "\n",
    "We can see that our dataset includes the age of each participant. We know that this is a continuous variable, so calculating the mean, median, range, interquartile range and variance would each be appropriate. We can do this in R as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note: $AGE in the below code means extract the AGE column from frmgham_1visit_data \n",
    "#      we can replace AGE with any of the other column names (e.g. $SYSBP)\n",
    "mean(frmgham_1visit_data$AGE) #calculates the mean\n",
    "median(frmgham_1visit_data$AGE) #calculates the median\n",
    "max(frmgham_1visit_data$AGE) - min(frmgham_1visit_data$AGE) #calculates the range\n",
    "quantile(frmgham_1visit_data$AGE, 0.25) #calculates lower quartile\n",
    "quantile(frmgham_1visit_data$AGE, 0.75) #calculates upper quartile\n",
    "var(frmgham_1visit_data$AGE) #calculates the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Activity:__ \n",
    "\n",
    "1) Run the above code cell. What does the output tell us? \n",
    "\n",
    "2) In the below empty code cell, write (or copy from above) code that replaces AGE with SYSBP. Click \"Run\" in the toolbar at the top of this page to execute your code. What does the output tell us about systolic blood pressure in this dataset? \n",
    "\n",
    "3) Modify the code again, to consider at least one other continuous variable. Interpret the results.\n",
    "\n",
    "4) Often, one reports only the mean or the median. Consider when each of these measures are more/less appropriate. (some notes on this at the end of this workshop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##You can write R code here, then click the \"Run\" button in the toolbar at the top to execute your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving beyond continuous data\n",
    "In the above, we computed measures of location and spread of continuous variables. Additionally, your digital transformation project might plan to collect discrete/categorical data. Clearly, computing the mean of a categorical variable wouldnt make sense. Therefore, for these variables we must turn to other summary statistics, such as mode (e.g. which category occured most often), or simple freuqnecies of occurence. \n",
    "\n",
    "We explore this in the below code for the variable \"SEX\". You will then have opportunity to adapt the code to consider other categorical variables in this dataset. \n",
    "\n",
    "Note that sex is coded in the data as 1 for men and 2 for women. Coding categorical variables as a particular integer is common in many health datasets (and is why robust data dictionaries are vital, so end-users of the data know what a \"2\" for sex means, for example); you can see a similar type of coding for variables such as DIABETES (0 = no, 1 = yes). You can see the coding of all variables on the pdf file on Moodle.\n",
    "\n",
    "With that being said, lets calculate the number of females in the data, and the corresponding percentage, by running the below code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(frmgham_1visit_data$SEX == 2) #this counts the number of times the SEX variable equals 2 (i.e. female)\n",
    "(sum(frmgham_1visit_data$SEX == 2) / nrow(frmgham_1visit_data)) * 100 \n",
    "#this last line of code takes the number of females, divides by the number of rows of data (i.e. \n",
    "#total number of participants), then times by 100 to get the %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output tells us that there were 2490 females in the dataset, which corresponds to 56.2% of the dataset (i.e. female is also the modal category). \n",
    "\n",
    "__Activity: In the below empty code cell, write (or copy from above) code that calculates the number of patitents with diabetes, and calculate the proportion of the dataset this corresponds to.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##You can write R code here, then click the \"Run\" button in the toolbar at the top to execute your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A picture tells a thousand words\n",
    "So far in this workshop, we have considered calculating single summary measures. However, as we have covered in other modules of this course, graphical displays can be very powerful, particularly for communication. Therefore, you might want to consider incorperating graphical plots into your statistical anaysis plans (for both DTP case studies, and generally in other projects). We will not take a deep-dive into this topic (which was covered in the Principles of Health Data Analytics module) but, as a recap, some plots you could consider are boxplots, histograms, and scatter plots. \n",
    "\n",
    "__Activity: Consider the following in terms of the next three code cells: what does the output tell us? Why was each type of plot used? What was the intended aim of each plot (in terms of what interpretation one would wish to gain prior to making the plots)?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot(frmgham_1visit_data$SYSBP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(frmgham_1visit_data$SYSBP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(frmgham_1visit_data$SYSBP, frmgham_1visit_data$AGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Optional Activity: You might wish to modify the above code (using the empty code cell below) to create similar plots for the other variables (feel free to experiment: google has many resources for plotting functions in R). In each case, consider carefully what the output is telling us (and therefore why you might want to condering planning on such plots at the design stage of writing a proposal)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##You can write R code here, then click the \"Run\" button in the toolbar at the top to execute your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about other data sources?\n",
    "__Activity: Above, the Framingham data was already structured in terms of columns of numerical data, where we could apply measures of location, spread or graphics. For the final part of this workshop, we would like you to consider alternative sources of data that were covered in the data-landscape lecture. For example, how would we apply the above concepts to the following types of data: (i) coded data, such as those within electronic health records, (ii) imaging data, (iii) unstructured data, such as free text (hint: think carefully about drawing common themes), (iv) social media data, and (v) data from wearable devices, such as smartphones. In each case, think carefully about how the underlying data would be structured, and thereby what pre-processing steps might be required to apply the concepts covered in this workshop. Post your answers on the forum for the Moodle for this week.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the workshop\n",
    "This workshop has been an introduction to how we can use descriptive statistics to summarise large datasets. Nearly all projects will include some form of descriptive statistics: we can only begin to see if a digital intervention has been successful if we can begin to explore the data the intervention has created. \n",
    "\n",
    "As such, you will usually need to consider what types of summary are relevant for your specific project at the design stage. In this workshop, we have explored what the end outputs from different descriptive statistics can tell us, thereby allowing insights into how one can plan for their use at the design stage of a project. For example, we can consider both (a) the type of variable, and (b) the measures of location and spread, to help in deciding which is relevant for each individual project. \n",
    "\n",
    "While the health data-landscape is vast (including structured, unstructured and patient-generated data, such as wearable devices), nearly all of the above methods can be applied (albeit with some pre-processing required, such as turning ICD-9 codes into different comorbidities, for example).\n",
    "\n",
    "While the above concepts might be familiar to many of you, it is worth noting that they are easily overlooked when designing a project, and even more so when it comes to analysing the data (it is very tempting to dive straight into main analyses when we recieve data!). However, descriptive statistics are a vital first step to understand the data (and find potential data errors). For example, in most cases, the first pargraph of a statistical analysis plan will include wording such as: \"continuous variables will be summarised using the mean/median and variance/interquarile range\". The impotance of planning how you will summarise/describe your data cannot be overstated.\n",
    "\n",
    "## Notes on some of the above activities\n",
    "One of the activities above asked you to consider when it is appropriate to use the mean over the median (and vice versa). The decision of which to use is often based on whether a variable is expected to be skewed. Skewed data have a distribution that is not symmetric, which means that the large values do not balance with the small values. For example, consider a dataset where we have collected length of hospital stay. Most people stay in a hospital for a few days. However, some patients have hospital stays for months on end. If you were to report the mean length of stay, this would be inflated by the few very large lengths of stay. Therefore, in this example, you would likely report the median length of hospital stay, which separates the higher half from the lower half. In general, however, most people report the mean unless you have a good reason for not doing so, such as when we have skewed data like in the hospital example here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
